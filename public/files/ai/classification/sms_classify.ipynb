{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression for SMS spam classification\n",
    "\n",
    "\n",
    "Each line of the data file `sms.txt`\n",
    "contains a label---either \"spam\" or \"ham\" (i.e. non-spam)---followed\n",
    "by a text message. Here are a few examples (line breaks added for readability):\n",
    "\n",
    "    ham     Ok lar... Joking wif u oni...\n",
    "    ham     Nah I don't think he goes to usf, he lives around here though\n",
    "    spam    Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005.\n",
    "            Text FA to 87121 to receive entry question(std txt rate)\n",
    "            T&C's apply 08452810075over18's\n",
    "    spam    WINNER!! As a valued network customer you have been\n",
    "            selected to receivea £900 prize reward! To claim\n",
    "            call 09061701461. Claim code KL341. Valid 12 hours only.\n",
    "\n",
    "To create features suitable for logistic regression, code is provided to do the following (using tools from the ``sklearn.feature_extraction.text``):\n",
    "\n",
    "* Convert words to lowercase.\n",
    "* Remove punctuation and special characters (but convert the \\$ and\n",
    "  £ symbols to special tokens and keep them, because these are useful for predicting spam).\n",
    "* Create a dictionary containing the 3000 words that appeared\n",
    "  most frequently in the entire set of messages.\n",
    "* Encode each message as a vector $\\mathbf{x}^{(i)} \\in\n",
    "  \\mathbb{R}^{3000}$. The entry $x^{(i)}_j$ is equal to the\n",
    "  number of times the $j$th word in the dictionary appears in that\n",
    "  message.\n",
    "* Discard some ham messages to have an\n",
    "  equal number of spam and ham messages.\n",
    "* Split data into a training set of 1000 messages and a\n",
    "  test set of 400 messages.\n",
    "  \n",
    "Follow the instructions below to complete the implementation. Your job will be to:\n",
    "\n",
    "* Learn $\\boldsymbol{\\theta}$ by gradient descent\n",
    "* Plot the cost history\n",
    "* Make predictions and report the accuracy on the test set\n",
    "* Test out the classifier on a few of your own text messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prep data\n",
    "This cell preps the data. Take a look to see how it works, and then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "\n",
    "from logistic_regression import logistic, cost_function, gradient_descent\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Preprocess the SMS Spam Collection data set\n",
    "#  \n",
    "#   https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
    "# \n",
    "# From Dan Sheldon\n",
    "\n",
    "numTrain    = 1000\n",
    "numTest     = 494\n",
    "numFeatures = 3000\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# Open the file\n",
    "f = codecs.open('sms.txt', encoding='utf-8')\n",
    "\n",
    "labels = []    # list of labels for each message\n",
    "docs   = []    # list of messages\n",
    "\n",
    "# Go through each line of file and extract the label and the message\n",
    "for line in f:\n",
    "    l, d= line.strip().split('\\t', 1)\n",
    "    labels.append(l)\n",
    "    docs.append(d)\n",
    "\n",
    "# This function will be called on each message to preprocess it\n",
    "def preprocess(doc):\n",
    "    # Replace all currency signs and some url patterns by special\n",
    "    # tokens. These are useful features.\n",
    "    doc = re.sub('[£$]', ' __currency__ ', doc)\n",
    "    doc = re.sub('\\://', ' __url__ ', doc)\n",
    "    doc = doc.lower() # convert to lower\n",
    "    return doc\n",
    "\n",
    "\n",
    "# This is the object that does the conversion from text to feature vectors\n",
    "vectorizer = CountVectorizer(max_features=numFeatures, preprocessor=preprocess)\n",
    "\n",
    "# Do the conversion (\"fit\" the transform from text to feature vector. \n",
    "#   later we will also \"apply\" the tranform on test messages)\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Convert labels to numbers: 1 = spam, 0 = ham\n",
    "y = np.array([l == 'spam' for l in labels]).astype('int')\n",
    "\n",
    "# The vectorizer returns sparse scipy arrays. Convert this back to a dense \n",
    "#   numpy array --- not as efficient but easier to work with\n",
    "X = X.toarray()\n",
    "m,n = X.shape\n",
    "\n",
    "# Add a column of ones\n",
    "X = np.column_stack([np.ones(m), X])\n",
    "\n",
    "# \n",
    "# Now massage and split into test/train\n",
    "# \n",
    "pos = np.nonzero(y == 1)[0]   # indices of positive training examples\n",
    "neg = np.nonzero(y == 0)[0]   # indices of negative training examples\n",
    "\n",
    "npos = len(pos)\n",
    "\n",
    "# Create a subset that has the same number of positive and negative examples\n",
    "subset = np.concatenate([pos, neg[0:len(pos)] ])\n",
    "\n",
    "# Randomly shuffle order of examples\n",
    "np.random.shuffle(subset)\n",
    "      \n",
    "X = X[subset,:]\n",
    "y = y[subset]\n",
    "\n",
    "# Split into test and train\n",
    "train = np.arange(numTrain)\n",
    "test  = numTrain + np.arange(numTest)\n",
    "\n",
    "X_train = X[train,:]\n",
    "y_train = y[train]\n",
    "\n",
    "X_test  = X[test,:]\n",
    "y_test  = y[test]\n",
    "\n",
    "# Extract the list of test documents\n",
    "test_docs = [docs[i] for i in subset[test]]\n",
    "\n",
    "# Extract the list of tokens (words) in the dictionary\n",
    "tokens = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train logistic regresion model\n",
    "Now train the logistic regression model. The comments summarize the relevant variables created by the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_train     contains information about the words within the training\n",
    "#             messages. the ith row represents the ith training message. \n",
    "#             for a particular text, the entry in the jth column tells\n",
    "#             you how many times the jth dictionary word appears in \n",
    "#             that message\n",
    "#\n",
    "# X_test      similar but for test set\n",
    "#\n",
    "# y_train     ith entry indicates whether message i is spam\n",
    "#\n",
    "# y_test      similar\n",
    "#\n",
    "\n",
    "m, n = X_train.shape\n",
    "\n",
    "theta = np.zeros(n)\n",
    "\n",
    "alpha = 0.01\n",
    "iters = 1500\n",
    "theta, J_history = gradient_descent(X_train, y_train, theta, alpha, iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(iters), J_history)\n",
    "plt.title('Cost over time')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions on test set\n",
    "Use the model fit in the previous cell to make predictions on the test set and compute the accuracy (percentage of messages in the test set that are classified correctly). You should be able to get accuracy above 95%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.56%\n"
     ]
    }
   ],
   "source": [
    "m_test, n_test = X_test.shape\n",
    "\n",
    "h_theta = lambda X_i: logistic(np.dot(theta.T, X_i))\n",
    "predictions = np.round(np.apply_along_axis(h_theta, 1, X_test))\n",
    "correct_classifications = predictions[predictions == y_test]\n",
    "\n",
    "print(\"Accuracy: %.2f\" % ((correct_classifications.shape[0] / predictions.shape[0]) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect model parameters\n",
    "Run this code to examine the model parameters you just learned. These parameters assign a postive or negative value to each word --- where positive values are words that tend to be spam and negative values are words that tend to be ham. Do they make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 spam words\n",
      "  +5.8450  ringtoneking\n",
      "  +4.8168  __currency__\n",
      "  +4.2248  text\n",
      "  +3.2305  txt\n",
      "  +3.0174  ringtone\n",
      "  +2.9719  service\n",
      "  +2.8033  message\n",
      "  +2.7762  150p\n",
      "  +2.6823  mobile\n",
      "  +2.6582  call\n",
      "\n",
      "Top 10 ham words\n",
      "  -3.4839  waiting\n",
      "  -2.5135  ok\n",
      "  -2.2588  later\n",
      "  -2.2045  so\n",
      "  -2.2045  what\n",
      "  -2.1207  ll\n",
      "  -1.9858  my\n",
      "  -1.9303  me\n",
      "  -1.7962  he\n",
      "  -1.7923  come\n"
     ]
    }
   ],
   "source": [
    "token_weights = theta[1:]\n",
    "\n",
    "def reverse(a):\n",
    "    return a[::-1]\n",
    "\n",
    "most_negative = np.argsort(token_weights)\n",
    "most_positive = reverse(most_negative)\n",
    "\n",
    "k = 10\n",
    "\n",
    "print('Top %d spam words' % k)\n",
    "for i in most_positive[0:k]:\n",
    "    print('  %+.4f  %s' % (token_weights[i], tokens[i]))\n",
    "\n",
    "print('\\nTop %d ham words' % k)\n",
    "for i in most_negative[0:k]:\n",
    "    print('  %+.4f  %s' % (token_weights[i], tokens[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction on new messages\n",
    "Type a few of your own messages in below and make predictions. Are they ham or spam? Do the predictions make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msg:  hey jennifer how was your weekend?\n",
      "prediction: ham\n",
      "\n",
      "msg:  REPLY: text 15003 to this number for free iPhone\n",
      "prediction: spam\n",
      "\n",
      "msg:  CONGRATS you won a raffle for Amazon $50 gift card!!\n",
      "prediction: spam\n",
      "\n",
      "msg:  can u pick up sum eggs from store after work\n",
      "prediction: ham\n",
      "\n",
      "msg:  dude i cant call u rn bc i got rly bad service\n",
      "prediction: ham\n",
      "\n",
      "msg:  text me later when ur free, i got a surprise for u\n",
      "prediction: ham\n",
      "\n",
      "msg:  can u text me when u get home\n",
      "prediction: ham\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_features(msg):\n",
    "    x = vectorizer.transform([msg]).toarray()\n",
    "    x = np.insert(x, 0, 1)\n",
    "    return x\n",
    "\n",
    "msg = u'Write your own text here...'\n",
    "x = extract_features(msg)  # this is the feature vector\n",
    "\n",
    "msgs = [\n",
    "    u'hey jennifer how was your weekend?', \n",
    "    u'REPLY: text 15003 to this number for free iPhone', \n",
    "    u'CONGRATS you won a raffle for Amazon $50 gift card!!', \n",
    "    u'can u pick up sum eggs from store after work',\n",
    "    u'dude i cant call u rn bc i got rly bad service',\n",
    "    u'text me later when ur free, i got a surprise for u',\n",
    "    u'can u text me when u get home'\n",
    "]\n",
    "for msg in msgs:\n",
    "    x = extract_features(msg)\n",
    "    print(\"msg: \", msg)\n",
    "    print(\"prediction: \", end=\"\")\n",
    "    print(\"ham\" if np.sum(np.dot(theta.T, x)) < 0 else \"spam\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "88738ecac3f5fe7d1162ad09de316cfd978b10d6a9bd8990569be6f1dc8a3b45"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
